{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing the necessary libraries and loading the malaria dataset from TFDS\n",
    "\n",
    "Since there is only a training set available (from documentation provided with dataset), I decided to shuffle the data and split it into training, validation & testing sets from the start. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-09T12:26:41.816299Z",
     "iopub.status.busy": "2021-07-09T12:26:41.815919Z",
     "iopub.status.idle": "2021-07-09T12:28:42.502505Z",
     "shell.execute_reply": "2021-07-09T12:28:42.501442Z",
     "shell.execute_reply.started": "2021-07-09T12:26:41.816218Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset Unknown size (download: Unknown size, generated: Unknown size, total: Unknown size) to C:\\Users\\al-kuba\\tensorflow_datasets\\malaria\\1.0.0...\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61583d4dbc8b4256acdaeccee5135c0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Completed...: 0 url [00:00, ? url/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e89f1780ea7e43cba5562a55ed1e0ac4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Size...: 0 MiB [00:00, ? MiB/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38d117168a6049d4a65d987d3c5854f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extraction completed...: 0 file [00:00, ? file/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ExtractError",
     "evalue": "Error while extracting C:\\Users\\al-kuba\\tensorflow_datasets\\downloads\\ceb.nlm.nih.gov_proj_malaria_cell_imagesCpSVVrJBQVm1EAGSYJgFN2ZUxCZtjRh76bGSL61Dxmg.zip to C:\\Users\\al-kuba\\tensorflow_datasets\\downloads\\extracted\\ZIP.ceb.nlm.nih.gov_proj_malaria_cell_imagesCpSVVrJBQVm1EAGSYJgFN2ZUxCZtjRh76bGSL61Dxmg.zip (file: cell_images\\Parasitized\\C100P61ThinF_IMG_20150918_144104_cell_162.png) : Failed to create a NewWriteableFile: C:\\Users\\al-kuba\\tensorflow_datasets\\downloads\\extracted\\ZIP.ceb.nlm.nih.gov_proj_malaria_cell_imagesCpSVVrJBQVm1EAGSYJgFN2ZUxCZtjRh76bGSL61Dxmg.zip.incomplete_c8826e2124ea4996bb2b8dfffb00ee58\\cell_images\\Parasitized\\C100P61ThinF_IMG_20150918_144104_cell_162.png : The system cannot find the path specified.\r\n; No such process\nOn windows, path lengths greater than 260 characters may result in an error. See the doc to remove the limitation: https://docs.python.org/3/using/windows.html#removing-the-max-path-limitation",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow_datasets\\core\\download\\extractor.py\u001b[0m in \u001b[0;36m_sync_extract\u001b[1;34m(self, from_path, method, to_path)\u001b[0m\n\u001b[0;32m     89\u001b[0m         \u001b[0mdst_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpath\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mto_path_tmp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mto_path_tmp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 90\u001b[1;33m         \u001b[0m_copy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdst_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     91\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow_datasets\\core\\download\\extractor.py\u001b[0m in \u001b[0;36m_copy\u001b[1;34m(src_file, dest_path)\u001b[0m\n\u001b[0;32m    125\u001b[0m         \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 126\u001b[1;33m       \u001b[0mdest_file\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    127\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\lib\\io\\file_io.py\u001b[0m in \u001b[0;36mwrite\u001b[1;34m(self, file_content)\u001b[0m\n\u001b[0;32m    100\u001b[0m     \u001b[1;34m\"\"\"Writes file_content to the file. Appends to the end of the file.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 101\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_prewrite_check\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    102\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_writable_file\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_bytes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_content\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\lib\\io\\file_io.py\u001b[0m in \u001b[0;36m_prewrite_check\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     85\u001b[0m                                            \"File isn't open for writing\")\n\u001b[1;32m---> 86\u001b[1;33m       self._writable_file = _pywrap_file_io.WritableFile(\n\u001b[0m\u001b[0;32m     87\u001b[0m           compat.as_bytes(self.__name), compat.as_bytes(self.__mode))\n",
      "\u001b[1;31mNotFoundError\u001b[0m: Failed to create a NewWriteableFile: C:\\Users\\al-kuba\\tensorflow_datasets\\downloads\\extracted\\ZIP.ceb.nlm.nih.gov_proj_malaria_cell_imagesCpSVVrJBQVm1EAGSYJgFN2ZUxCZtjRh76bGSL61Dxmg.zip.incomplete_c8826e2124ea4996bb2b8dfffb00ee58\\cell_images\\Parasitized\\C100P61ThinF_IMG_20150918_144104_cell_162.png : The system cannot find the path specified.\r\n; No such process",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mExtractError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-90c52521eb8e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m [train_ds, valid_ds, test_ds] , info = tfds.load(\"malaria\", \n\u001b[0m\u001b[0;32m      7\u001b[0m                                                  \u001b[0msplit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'train[:70%]'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'train[70%:85%]'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'train[85%:]'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m                                                  \u001b[0mshuffle_files\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow_datasets\\core\\load.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(name, split, data_dir, batch_size, shuffle_files, download, as_supervised, decoders, read_config, with_info, builder_kwargs, download_and_prepare_kwargs, as_dataset_kwargs, try_gcs)\u001b[0m\n\u001b[0;32m    331\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mdownload\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    332\u001b[0m     \u001b[0mdownload_and_prepare_kwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdownload_and_prepare_kwargs\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 333\u001b[1;33m     \u001b[0mdbuilder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdownload_and_prepare\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mdownload_and_prepare_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    334\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    335\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mas_dataset_kwargs\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow_datasets\\core\\dataset_builder.py\u001b[0m in \u001b[0;36mdownload_and_prepare\u001b[1;34m(self, download_dir, download_config)\u001b[0m\n\u001b[0;32m    437\u001b[0m           \u001b[1;31m# Old version of TF are not os.PathLike compatible\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    438\u001b[0m           \u001b[1;32mwith\u001b[0m \u001b[0mtf_compat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmock_gfile_pathlike\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 439\u001b[1;33m             self._download_and_prepare(\n\u001b[0m\u001b[0;32m    440\u001b[0m                 \u001b[0mdl_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdl_manager\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    441\u001b[0m                 \u001b[0mdownload_config\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdownload_config\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow_datasets\\core\\dataset_builder.py\u001b[0m in \u001b[0;36m_download_and_prepare\u001b[1;34m(self, dl_manager, download_config)\u001b[0m\n\u001b[0;32m   1131\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         \u001b[0moptional_pipeline_kwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1133\u001b[1;33m       split_generators = self._split_generators(  # pylint: disable=unexpected-keyword-arg\n\u001b[0m\u001b[0;32m   1134\u001b[0m           dl_manager, **optional_pipeline_kwargs)\n\u001b[0;32m   1135\u001b[0m       \u001b[1;31m# TODO(tfds): Could be removed once all datasets are migrated.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow_datasets\\image_classification\\malaria.py\u001b[0m in \u001b[0;36m_split_generators\u001b[1;34m(self, dl_manager)\u001b[0m\n\u001b[0;32m     71\u001b[0m     \u001b[1;34m\"\"\"Define Splits.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m     \u001b[0mpath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdl_manager\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdownload_and_extract\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_URL\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m     return [\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow_datasets\\core\\download\\download_manager.py\u001b[0m in \u001b[0;36mdownload_and_extract\u001b[1;34m(self, url_or_urls)\u001b[0m\n\u001b[0;32m    635\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_downloader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    636\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extractor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 637\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_map_promise\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_download_extract\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl_or_urls\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    638\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    639\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow_datasets\\core\\download\\download_manager.py\u001b[0m in \u001b[0;36m_map_promise\u001b[1;34m(map_fn, all_inputs)\u001b[0m\n\u001b[0;32m    776\u001b[0m   \u001b[1;34m\"\"\"Map the function into each element and resolve the promise.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    777\u001b[0m   \u001b[0mall_promises\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmap_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mall_inputs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Apply the function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 778\u001b[1;33m   \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mall_promises\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Wait promises\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    779\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\util\\nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[1;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[0;32m    633\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    634\u001b[0m   return pack_sequence_as(\n\u001b[1;32m--> 635\u001b[1;33m       \u001b[0mstructure\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    636\u001b[0m       expand_composites=expand_composites)\n\u001b[0;32m    637\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\util\\nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    633\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    634\u001b[0m   return pack_sequence_as(\n\u001b[1;32m--> 635\u001b[1;33m       \u001b[0mstructure\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    636\u001b[0m       expand_composites=expand_composites)\n\u001b[0;32m    637\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow_datasets\\core\\download\\download_manager.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(p)\u001b[0m\n\u001b[0;32m    776\u001b[0m   \u001b[1;34m\"\"\"Map the function into each element and resolve the promise.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    777\u001b[0m   \u001b[0mall_promises\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmap_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mall_inputs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Apply the function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 778\u001b[1;33m   \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mall_promises\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Wait promises\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    779\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf\\lib\\site-packages\\promise\\promise.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    510\u001b[0m         \u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_target\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    511\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mDEFAULT_TIMEOUT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 512\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_target_settled_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_raise\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    513\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    514\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_target_settled_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_raise\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf\\lib\\site-packages\\promise\\promise.py\u001b[0m in \u001b[0;36m_target_settled_value\u001b[1;34m(self, _raise)\u001b[0m\n\u001b[0;32m    514\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_target_settled_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_raise\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    515\u001b[0m         \u001b[1;31m# type: (bool) -> Any\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 516\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_target\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_settled_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_raise\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    517\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    518\u001b[0m     \u001b[0m_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_reason\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_target_settled_value\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf\\lib\\site-packages\\promise\\promise.py\u001b[0m in \u001b[0;36m_settled_value\u001b[1;34m(self, _raise)\u001b[0m\n\u001b[0;32m    224\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0m_raise\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    225\u001b[0m                 \u001b[0mraise_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fulfillment_handler0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 226\u001b[1;33m                 \u001b[0mreraise\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraise_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mraise_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_traceback\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    227\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fulfillment_handler0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    228\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf\\lib\\site-packages\\six.py\u001b[0m in \u001b[0;36mreraise\u001b[1;34m(tp, value, tb)\u001b[0m\n\u001b[0;32m    701\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mtb\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    702\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 703\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    704\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    705\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf\\lib\\site-packages\\promise\\promise.py\u001b[0m in \u001b[0;36mhandle_future_result\u001b[1;34m(future)\u001b[0m\n\u001b[0;32m    842\u001b[0m         \u001b[1;31m# type: (Any) -> None\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    843\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 844\u001b[1;33m             \u001b[0mresolve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfuture\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    845\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    846\u001b[0m             \u001b[0mtb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf\\lib\\concurrent\\futures\\_base.py\u001b[0m in \u001b[0;36mresult\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    430\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mCancelledError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    431\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mFINISHED\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 432\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    433\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    434\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf\\lib\\concurrent\\futures\\_base.py\u001b[0m in \u001b[0;36m__get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    386\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__get_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    387\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 388\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    389\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    390\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf\\lib\\concurrent\\futures\\thread.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 57\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     58\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfuture\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow_datasets\\core\\download\\extractor.py\u001b[0m in \u001b[0;36m_sync_extract\u001b[1;34m(self, from_path, method, to_path)\u001b[0m\n\u001b[0;32m    100\u001b[0m             \u001b[1;34m'https://docs.python.org/3/using/windows.html#removing-the-max-path-limitation'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m         )\n\u001b[1;32m--> 102\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mExtractError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    103\u001b[0m     \u001b[1;31m# `tf.io.gfile.Rename(overwrite=True)` doesn't work for non empty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m     \u001b[1;31m# directories, so delete destination first, if it already exists.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mExtractError\u001b[0m: Error while extracting C:\\Users\\al-kuba\\tensorflow_datasets\\downloads\\ceb.nlm.nih.gov_proj_malaria_cell_imagesCpSVVrJBQVm1EAGSYJgFN2ZUxCZtjRh76bGSL61Dxmg.zip to C:\\Users\\al-kuba\\tensorflow_datasets\\downloads\\extracted\\ZIP.ceb.nlm.nih.gov_proj_malaria_cell_imagesCpSVVrJBQVm1EAGSYJgFN2ZUxCZtjRh76bGSL61Dxmg.zip (file: cell_images\\Parasitized\\C100P61ThinF_IMG_20150918_144104_cell_162.png) : Failed to create a NewWriteableFile: C:\\Users\\al-kuba\\tensorflow_datasets\\downloads\\extracted\\ZIP.ceb.nlm.nih.gov_proj_malaria_cell_imagesCpSVVrJBQVm1EAGSYJgFN2ZUxCZtjRh76bGSL61Dxmg.zip.incomplete_c8826e2124ea4996bb2b8dfffb00ee58\\cell_images\\Parasitized\\C100P61ThinF_IMG_20150918_144104_cell_162.png : The system cannot find the path specified.\r\n; No such process\nOn windows, path lengths greater than 260 characters may result in an error. See the doc to remove the limitation: https://docs.python.org/3/using/windows.html#removing-the-max-path-limitation"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "[train_ds, valid_ds, test_ds] , info = tfds.load(\"malaria\", \n",
    "                                                 split = ['train[:70%]', 'train[70%:85%]', 'train[85%:]'], \n",
    "                                                 shuffle_files=True, \n",
    "                                                 with_info=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring the data\n",
    "\n",
    "\n",
    "\n",
    "*   Looking at the structure of the dataset which shows that images are not a consistent size\n",
    "*   Display the number of training samples provided in the overall dataset\n",
    "\n",
    "\n",
    "*   Display the number of classes in the dataset\n",
    "*   Dispaly the labels of the classes in the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-09T12:28:42.505225Z",
     "iopub.status.busy": "2021-07-09T12:28:42.504760Z",
     "iopub.status.idle": "2021-07-09T12:28:42.513849Z",
     "shell.execute_reply": "2021-07-09T12:28:42.512040Z",
     "shell.execute_reply.started": "2021-07-09T12:28:42.505173Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Training Dataset structure --> \" + str(train_ds))\n",
    "print(\"Dataset Info --> \" + str(info))\n",
    "print (\"******************************************\\n\")\n",
    "\n",
    "ds_size = info.splits[\"train\"].num_examples\n",
    "ds_names = info.features[\"label\"].names\n",
    "ds_classes = info.features[\"label\"].num_classes\n",
    "\n",
    "print (\"******************************************\\n\")\n",
    "print (\"Num of training samples: \" + str(ds_size))\n",
    "print (\"Classes: \" + str(ds_names))\n",
    "print (\"Num of Classes: \" + str(ds_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display a few samples just to make sure that they are all different sizes which checks out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-09T12:28:42.515902Z",
     "iopub.status.busy": "2021-07-09T12:28:42.515509Z",
     "iopub.status.idle": "2021-07-09T12:28:42.585545Z",
     "shell.execute_reply": "2021-07-09T12:28:42.584661Z",
     "shell.execute_reply.started": "2021-07-09T12:28:42.515857Z"
    }
   },
   "outputs": [],
   "source": [
    "for ex in train_ds.take(2):\n",
    "    print (ex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the provided show_examples method by tfds to see a few of the images with their labels & values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-09T12:28:42.587438Z",
     "iopub.status.busy": "2021-07-09T12:28:42.586932Z",
     "iopub.status.idle": "2021-07-09T12:28:42.591301Z",
     "shell.execute_reply": "2021-07-09T12:28:42.590194Z",
     "shell.execute_reply.started": "2021-07-09T12:28:42.587399Z"
    }
   },
   "outputs": [],
   "source": [
    "# This works in Collab and my own personal Jupyter Notebook but not sure why it doesn't work in Kaggle, any ideas??\n",
    "disp = tfds.show_examples(train_ds, info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting the global variables that will be used throughout the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-09T12:28:42.594887Z",
     "iopub.status.busy": "2021-07-09T12:28:42.594318Z",
     "iopub.status.idle": "2021-07-09T12:28:42.601927Z",
     "shell.execute_reply": "2021-07-09T12:28:42.600911Z",
     "shell.execute_reply.started": "2021-07-09T12:28:42.594850Z"
    }
   },
   "outputs": [],
   "source": [
    "NEW_HEIGHT = 200\n",
    "NEW_WIDTH = 200\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A function to resize the images to a consistent size that can be used by the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-09T12:28:42.604000Z",
     "iopub.status.busy": "2021-07-09T12:28:42.603533Z",
     "iopub.status.idle": "2021-07-09T12:28:42.613527Z",
     "shell.execute_reply": "2021-07-09T12:28:42.612662Z",
     "shell.execute_reply.started": "2021-07-09T12:28:42.603960Z"
    }
   },
   "outputs": [],
   "source": [
    "def fix_image(ds):\n",
    "  ds[\"image\"] = tf.image.convert_image_dtype(ds[\"image\"], tf.float32)\n",
    "  ds[\"image\"] = tf.image.resize_with_crop_or_pad(ds[\"image\"], NEW_HEIGHT, NEW_WIDTH)\n",
    "  return ds[\"image\"], ds[\"label\"];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resizing the 3 datasets that will be used to train, validate and test our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-09T12:28:42.615429Z",
     "iopub.status.busy": "2021-07-09T12:28:42.614863Z",
     "iopub.status.idle": "2021-07-09T12:28:43.110325Z",
     "shell.execute_reply": "2021-07-09T12:28:43.109550Z",
     "shell.execute_reply.started": "2021-07-09T12:28:42.615384Z"
    }
   },
   "outputs": [],
   "source": [
    "fixed_train_ds = train_ds.map(fix_image).batch(BATCH_SIZE)\n",
    "fixed_valid_ds = valid_ds.map(fix_image).batch(BATCH_SIZE)\n",
    "fixed_test_ds = test_ds.map(fix_image).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Displaying some of the converted images to see that their size is consistent and that we didn't corrupt the data in any way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-09T12:28:43.112000Z",
     "iopub.status.busy": "2021-07-09T12:28:43.111665Z",
     "iopub.status.idle": "2021-07-09T12:28:44.447421Z",
     "shell.execute_reply": "2021-07-09T12:28:44.446489Z",
     "shell.execute_reply.started": "2021-07-09T12:28:43.111965Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_image, batch_label = next(iter(fixed_train_ds))\n",
    "\n",
    "plt.figure(figsize= (16,12))\n",
    "\n",
    "for i in range(16):\n",
    "    ax = plt.subplot(4,4,i+1)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    plt.imshow(batch_image[i])\n",
    "    if batch_label[i]:\n",
    "        plt.title(ds_names[1])\n",
    "    else:\n",
    "        plt.title(ds_names[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the model\n",
    "- Create the input layer\n",
    "- Create the convolutional/Pooling layers\n",
    "  * They will half the filters at every layer the convulational layers increas\n",
    "  * The minimum # of filters will be 2 if there are too many layers\n",
    "- Falttening the layers so we can send them to the Dense layers\n",
    "- Create the dense layers\n",
    "  * A BatchNormalization Layer\n",
    "  * A Dense Layer\n",
    "  * A Dropout Layer\n",
    "- Final Dense layer with 1 unit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-09T12:28:44.448985Z",
     "iopub.status.busy": "2021-07-09T12:28:44.448580Z",
     "iopub.status.idle": "2021-07-09T12:28:44.464957Z",
     "shell.execute_reply": "2021-07-09T12:28:44.464010Z",
     "shell.execute_reply.started": "2021-07-09T12:28:44.448946Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_model (dropout=0.25, input_shape=[NEW_HEIGHT, NEW_WIDTH, 3]):\n",
    "    model = keras.models.Sequential()\n",
    "\n",
    "    model.add(keras.layers.InputLayer(input_shape=input_shape))\n",
    "    \n",
    "    model.add(keras.layers.Conv2D(32,3, padding=\"same\"))\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    model.add(keras.layers.ReLU())            \n",
    "    model.add(keras.layers.MaxPooling2D(2,2, padding=\"same\"))\n",
    "\n",
    "    model.add(keras.layers.Conv2D(64,3, padding=\"same\"))\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    model.add(keras.layers.ReLU())            \n",
    "    model.add(keras.layers.MaxPooling2D(2,2, padding=\"same\"))\n",
    "\n",
    "    model.add(keras.layers.Conv2D(128,3, padding=\"same\"))\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    model.add(keras.layers.ReLU())            \n",
    "    model.add(keras.layers.MaxPooling2D(2,2, padding=\"same\"))\n",
    "\n",
    "    model.add(keras.layers.Conv2D(256,3, padding=\"same\"))\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    model.add(keras.layers.ReLU())            \n",
    "    model.add(keras.layers.MaxPooling2D(2,2, padding=\"same\"))\n",
    "\n",
    "    model.add(keras.layers.Flatten())\n",
    "\n",
    "    model.add(keras.layers.Dense(256, activation='relu'))\n",
    "    model.add(keras.layers.Dropout(dropout))\n",
    "    \n",
    "    model.add(keras.layers.Dense(128, activation='relu'))\n",
    "    model.add(keras.layers.Dropout(dropout))\n",
    "\n",
    "    model.add(keras.layers.Dense(64, activation='relu'))\n",
    "    model.add(keras.layers.Dropout(dropout))\n",
    "\n",
    "    model.add(keras.layers.Dense(32, activation='relu'))\n",
    " \n",
    "    model.add(keras.layers.Dense(1, activation=\"sigmoid\"))\n",
    "    model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", \n",
    "                  metrics=[tf.metrics.AUC(), \"accuracy\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Model\n",
    "\n",
    "Creating 2 callbacks to save the best model and to stop training if its stop improving after 5 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-09T12:28:44.467433Z",
     "iopub.status.busy": "2021-07-09T12:28:44.466859Z",
     "iopub.status.idle": "2021-07-09T12:36:22.420531Z",
     "shell.execute_reply": "2021-07-09T12:36:22.419707Z",
     "shell.execute_reply.started": "2021-07-09T12:28:44.467388Z"
    }
   },
   "outputs": [],
   "source": [
    "checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\"malaria_tfds.h5\", save_best_only=True)\n",
    "\n",
    "earlystop_cb = tf.keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True)\n",
    "\n",
    "model = build_model()\n",
    "history = model.fit(fixed_train_ds, epochs=40, validation_data=fixed_valid_ds,  callbacks=[checkpoint_cb, earlystop_cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating the model against the Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-09T12:36:22.422397Z",
     "iopub.status.busy": "2021-07-09T12:36:22.422004Z",
     "iopub.status.idle": "2021-07-09T12:36:26.853478Z",
     "shell.execute_reply": "2021-07-09T12:36:26.852530Z",
     "shell.execute_reply.started": "2021-07-09T12:36:22.422355Z"
    }
   },
   "outputs": [],
   "source": [
    "model.load_weights(\"malaria_tfds.h5\")\n",
    "model.evaluate(fixed_test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-09T12:36:26.855180Z",
     "iopub.status.busy": "2021-07-09T12:36:26.854809Z",
     "iopub.status.idle": "2021-07-09T12:36:27.287763Z",
     "shell.execute_reply": "2021-07-09T12:36:27.286810Z",
     "shell.execute_reply.started": "2021-07-09T12:36:26.855143Z"
    }
   },
   "outputs": [],
   "source": [
    "auc = history.history[\"auc\"]\n",
    "val_auc = history.history[\"val_auc\"]\n",
    "\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs_range = range(len(history.history[\"auc\"]))\n",
    "\n",
    "plt.figure(figsize=(16, 12))\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.plot(epochs_range, auc, label='Training AUC')\n",
    "plt.plot(epochs_range, val_auc, label='Validation AUC')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation AUC')\n",
    "\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.plot(epochs_range, loss, label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "* AUC is typically a better metric for logistic regression problems since the data could be skeweed towards one class due to natural occurence but the documentation provided in this dataset showed that both classes were equally represented so I decided to include accuracy also and let the numbers speak for themselves\n",
    "* Regardless of which metric you use have a 95%+ AUC/Accurray would typically hint at some kind of overfitting but in this case I think the \"clean\" data played a major role in the outcome.  In real life you probably wouldn't get such a clean labelled data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
